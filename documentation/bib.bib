@Comment @inproceedings{zhu2016generative,
@Comment   title={Generative Visual Manipulation on the Natural Image Manifold},
@Comment   author={Zhu, Jun-Yan and Kr{\"a}henb{\"u}hl, Philipp and Shechtman,
@Comment                   Eli and Efros, Alexei A.},
@Comment   booktitle={Proceedings of European Conference on Computer Vision (ECCV)},
@Comment   year={2016}
@Comment }
                  
@article{ZhuJun-Yan2016GVMo,
abstract = {Realistic image manipulation is challenging because it
                  requires modifying the image appearance in a
                  user-controlled way, while preserving the realism of
                  the result. Unless the user has considerable
                  artistic skill, it is easy to "fall off" the
                  manifold of natural images while editing. In this
                  paper, we propose to learn the natural image
                  manifold directly from data using a generative
                  adversarial neural network. We then define a class
                  of image editing operations, and constrain their
                  output to lie on that learned manifold at all
                  times. The model automatically adjusts the output
                  keeping all edits as realistic as possible. All our
                  manipulations are expressed in terms of constrained
                  optimization and are applied in near-real time. We
                  evaluate our algorithm on the task of realistic
                  photo manipulation of shape and color. The presented
                  method can further be used for changing one image to
                  look like the other, as well as generating novel
                  imagery from scratch based on user's scribbles.},
year = "2016",
title = "Generative Visual Manipulation on the Natural Image
                  Manifold",
author = "Zhu, Jun-Yan and Krähenbühl, Philipp and Shechtman, Eli and
                  Efros, Alexei A.",
keywords = "Computer Science - Computer Vision And Pattern
                  Recognition",
month = "September",
}



@Comment @misc{Kim2017,
@Comment Author = {Taeksoo Kim and Moonsu Cha and Hyunsoo Kim and Jung Kwon Lee and Jiwon Kim},
@Comment Title = {Learning to Discover Cross-Domain Relations with Generative Adversarial Networks},
@Comment Year = {2017},
@Comment Eprint = {arXiv:1703.05192},
@Comment }

@article{KimTaeksoo2017LtDC,
abstract = "While humans easily recognize relations between data from
                  different domains without any supervision, learning
                  to automatically discover them is in general very
                  challenging and needs many ground-truth pairs that
                  illustrate the relations. To avoid costly pairing,
                  we address the task of discovering cross-domain
                  relations given unpaired data. We propose a method
                  based on generative adversarial networks that learns
                  to discover relations between different domains
                  (DiscoGAN). Using the discovered relations, our
                  proposed network successfully transfers style from
                  one domain to another while preserving key
                  attributes such as orientation and face
                  identity. Source code for official implementation is
                  publicly available
                  https://github.com/SKTBrain/DiscoGAN",
year = "2017",
title = "Learning to Discover Cross-Domain Relations with Generative
                  Adversarial Networks",
author = "Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung
                  Kwon and Kim, Jiwon",
keywords = "Computer Science - Computer Vision And Pattern
                  Recognition",
month = "March",
}



@Comment @misc{Goodfellow2014,
@Comment Author = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and
@Comment                   Bing Xu and David Warde-Farley and Sherjil Ozair and
@Comment                   Aaron Courville and Yoshua Bengio},
@Comment Title = {Generative Adversarial Networks},
@Comment Year = {2014},
@Comment Eprint = {arXiv:1406.2661},
@Comment }

@article{GoodfellowIanJ.2014GAN,
abstract = "We propose a new framework for estimating generative
                  models via an adversarial process, in which we
                  simultaneously train two models: a generative model
                  G that captures the data distribution, and a
                  discriminative model D that estimates the
                  probability that a sample came from the training
                  data rather than G. The training procedure for G is
                  to maximize the probability of D making a
                  mistake. This framework corresponds to a minimax
                  two-player game. In the space of arbitrary functions
                  G and D, a unique solution exists, with G recovering
                  the training data distribution and D equal to 1/2
                  everywhere. In the case where G and D are defined by
                  multilayer perceptrons, the entire system can be
                  trained with backpropagation. There is no need for
                  any Markov chains or unrolled approximate inference
                  networks during either training or generation of
                  samples. Experiments demonstrate the potential of
                  the framework through qualitative and quantitative
                  evaluation of the generated samples.",
year = "2014",
title = "Generative Adversarial Networks",
author = "Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi
                  and Xu, Bing and Warde-Farley, David and Ozair,
                  Sherjil and Courville, Aaron and Bengio, Yoshua",
keywords = "Statistics - Machine Learning ; Computer Science -
                  Learning",
month = "June",
}



@Comment @misc{Overview,
@Comment Author = {Antonia Creswell and Tom White and Vincent Dumoulin and Kai
@Comment                   Arulkumaran and Biswa Sengupta and Anil A Bharath},
@Comment Title = {Generative Adversarial Networks: An Overview},
@Comment Year = {2017},
@Comment Eprint = {arXiv:1710.07035},
@Comment }
                  
@article{CreswellAntonia2017GANA,
abstract = "Generative adversarial networks (GANs) provide a way to
                  learn deep representations without extensively
                  annotated training data. They achieve this through
                  deriving backpropagation signals through a
                  competitive process involving a pair of
                  networks. The representations that can be learned by
                  GANs may be used in a variety of applications,
                  including image synthesis, semantic image editing,
                  style transfer, image super-resolution and
                  classification. The aim of this review paper is to
                  provide an overview of GANs for the signal
                  processing community, drawing on familiar analogies
                  and concepts where possible. In addition to
                  identifying different methods for training and
                  constructing GANs, we also point to remaining
                  challenges in their theory and application.",
year = "2017",
title = "Generative Adversarial Networks: An Overview",
author = "Creswell, Antonia and White, Tom and Dumoulin, Vincent and
                  Arulkumaran, Kai and Sengupta, Biswa and Bharath,
                  Anil A",
keywords = "Computer Science - Computer Vision And Pattern
                  Recognition",
month = "October",
}



@Comment @misc{SuperRes,
@Comment Author = {Christian Ledig and Lucas Theis and Ferenc Huszar and Jose
@Comment                   Caballero and Andrew Cunningham and Alejandro Acosta
@Comment                   and Andrew Aitken and Alykhan Tejani and Johannes
@Comment                   Totz and Zehan Wang and Wenzhe Shi},
@Comment Title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
@Comment Year = {2016},
@Comment Eprint = {arXiv:1609.04802},
@Comment }
                  
@article{LedigChristian2016PSIS,
abstract = "Despite the breakthroughs in accuracy and speed of single
                  image super-resolution using faster and deeper
                  convolutional neural networks, one central problem
                  remains largely unsolved: how do we recover the
                  finer texture details when we super-resolve at large
                  upscaling factors? The behavior of
                  optimization-based super-resolution methods is
                  principally driven by the choice of the objective
                  function. Recent work has largely focused on
                  minimizing the mean squared reconstruction
                  error. The resulting estimates have high peak
                  signal-to-noise ratios, but they are often lacking
                  high-frequency details and are perceptually
                  unsatisfying in the sense that they fail to match
                  the fidelity expected at the higher resolution. In
                  this paper, we present SRGAN, a generative
                  adversarial network (GAN) for image super-resolution
                  (SR). To our knowledge, it is the first framework
                  capable of inferring photo-realistic natural images
                  for 4x upscaling factors. To achieve this, we
                  propose a perceptual loss function which consists of
                  an adversarial loss and a content loss. The
                  adversarial loss pushes our solution to the natural
                  image manifold using a discriminator network that is
                  trained to differentiate between the super-resolved
                  images and original photo-realistic images. In
                  addition, we use a content loss motivated by
                  perceptual similarity instead of similarity in pixel
                  space. Our deep residual network is able to recover
                  photo-realistic textures from heavily downsampled
                  images on public benchmarks. An extensive
                  mean-opinion-score (MOS) test shows hugely
                  significant gains in perceptual quality using
                  SRGAN. The MOS scores obtained with SRGAN are closer
                  to those of the original high-resolution images than
                  to those obtained with any state-of-the-art
                  method.",
year = "2016",
title = "Photo-Realistic Single Image Super-Resolution Using a
                  Generative Adversarial Network",
author = "Ledig, Christian and Theis, Lucas and Huszar, Ferenc and
                  Caballero, Jose and Cunningham, Andrew and Acosta,
                  Alejandro and Aitken, Andrew and Tejani, Alykhan and
                  Totz, Johannes and Wang, Zehan and Shi, Wenzhe",
keywords = "Computer Science - Computer Vision And Pattern Recognition
                  ; Statistics - Machine Learning",
month = "September",
}



@Comment @misc{MakeUpGAN,
@Comment Author = {Yi Li and Lingxiao Song and Xiang Wu and Ran He and Tieniu Tan},
@Comment Title = {Anti-Makeup: Learning A Bi-Level Adversarial Network for Makeup-Invariant Face Verification},
@Comment Year = {2017},
@Comment Eprint = {arXiv:1709.03654},
@Comment }

@article{CreswellAntonia2017GANA,
abstract = "Generative adversarial networks (GANs) provide a way to
                  learn deep representations without extensively
                  annotated training data. They achieve this through
                  deriving backpropagation signals through a
                  competitive process involving a pair of
                  networks. The representations that can be learned by
                  GANs may be used in a variety of applications,
                  including image synthesis, semantic image editing,
                  style transfer, image super-resolution and
                  classification. The aim of this review paper is to
                  provide an overview of GANs for the signal
                  processing community, drawing on familiar analogies
                  and concepts where possible. In addition to
                  identifying different methods for training and
                  constructing GANs, we also point to remaining
                  challenges in their theory and application.",
year = "2017",
title = "Generative Adversarial Networks: An Overview",
author = "Creswell, Antonia and White, Tom and Dumoulin, Vincent and
                  Arulkumaran, Kai and Sengupta, Biswa and Bharath,
                  Anil A",
keywords = "Computer Science - Computer Vision And Pattern
                  Recognition",
month = "October",
}



@Comment @misc{VGG,
@Comment Author = {Karen Simonyan and Andrew Zisserman},
@Comment Title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
@Comment Year = {2014},
@Comment Eprint = {arXiv:1409.1556},
@Comment }
                  
@article{SimonyanKaren2014VDCN,
abstract = "In this work we investigate the effect of the
                  convolutional network depth on its accuracy in the
                  large-scale image recognition setting. Our main
                  contribution is a thorough evaluation of networks of
                  increasing depth using an architecture with very
                  small (3x3) convolution filters, which shows that a
                  significant improvement on the prior-art
                  configurations can be achieved by pushing the depth
                  to 16-19 weight layers. These findings were the
                  basis of our ImageNet Challenge 2014 submission,
                  where our team secured the first and the second
                  places in the localisation and classification tracks
                  respectively. We also show that our representations
                  generalise well to other datasets, where they
                  achieve state-of-the-art results. We have made our
                  two best-performing ConvNet models publicly
                  available to facilitate further research on the use
                  of deep visual representations in computer vision.",
year = "2014",
title = "Very Deep Convolutional Networks for Large-Scale Image
                  Recognition",
author = "Simonyan, Karen and Zisserman, Andrew",
keywords = "Computer Science - Computer Vision And Pattern
                  Recognition",
month = "September",
}



@Comment @inproceedings{imagenet_cvpr09,
@Comment         AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
@Comment         TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
@Comment         BOOKTITLE = {CVPR09},
@Comment         YEAR = {2009},
@Comment         BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}

@inproceedings{JiaDengRichard2009IAlh,
issn = "1063-6919",
abstract = "The explosion of image data on the Internet has the
                  potential to foster more sophisticated and robust
                  models and algorithms to index, retrieve, organize
                  and interact with images and multimedia data. But
                  exactly how such data can be harnessed and organized
                  remains a critical problem. We introduce here a new
                  database called &#x201C;ImageNet&#x201D;, a
                  large-scale ontology of images built upon the
                  backbone of the WordNet structure. ImageNet aims to
                  populate the majority of the 80,000 synsets of
                  WordNet with an average of 500-1000 clean and full
                  resolution images. This will result in tens of
                  millions of annotated images organized by the
                  semantic hierarchy of WordNet. This paper offers a
                  detailed analysis of ImageNet in its current state:
                  12 subtrees with 5247 synsets and 3.2 million images
                  in total. We show that ImageNet is much larger in
                  scale and diversity and much more accurate than the
                  current image datasets. Constructing such a
                  large-scale database is a challenging task. We
                  describe the data collection scheme with Amazon
                  Mechanical Turk. Lastly, we illustrate the
                  usefulness of ImageNet through three simple
                  applications in object recognition, image
                  classification and automatic object clustering. We
                  hope that the scale, accuracy, diversity and
                  hierarchical structure of ImageNet can offer
                  unparalleled opportunities to researchers in the
                  computer vision community and beyond.",
journal = "Computer Vision and Pattern Recognition, 2009. CVPR
                  2009. IEEE Conference on",
pages = "248--255",
publisher = "IEEE",
isbn = "978-1-4244-3992-8",
year = "2009",
title = "ImageNet: A large-scale hierarchical image database",
language = "eng",
author = "Jia Deng, Richard and Wei Dong, Richard and Socher, Richard
                  and Li-Jia Li, Richard and Kai Li, Richard and Li
                  Fei-Fei, Richard",
keywords = "Computing and Processing ; Signal Processing and
                  Analysis",
month = "June",
}




@misc{pascal-voc-2012,
	author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
	title = "The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)} {R}esults",
	howpublished = "http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html"
	}

@InProceedings{Agustsson_2017_CVPR_Workshops,
	author = {Agustsson, Eirikur and Timofte, Radu},
	title = {NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
	month = {July},
	year = {2017}
} 