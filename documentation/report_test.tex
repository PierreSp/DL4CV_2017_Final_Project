\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{tgbonum}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{subcaption}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

\graphicspath{{results/}}
\newlength{\imagewidth}


\begin{document}

%%%%%%%%% TITLE
\title{Super-Resolution with GANs}

\author{Nathanael Bosch\\
{\tt\small nathanael.bosch@tum.de}
\and
Thomas Grassinger\\
{\tt\small thomas.grassinger@tum.de}
\and
Jonas Kipfstuhl\\
{\tt\small jonas.kipfstuhl@tum.de}
\and
Pierre Springer\\
{\tt\small pierre.springer@tum.de}
%\and
%Team Member 5\\
%{\tt\small fifth@i1.org}
}


\maketitle
%\thispagestyle{empty}

\section{Introduction}
We call super-resolution (SR) the task of estimating a high-resolution
(HR) image from its low-resolution (LR) counterpart. Recent work with
optimization-based methods largely focuses on minimizing the mean
squared reconstruction error, which results in high peak
signal--to--noise ratios (PSNR) but very smooth pictures.

The authors of the paper\cite{LedigChristian2016PSIS} propose using a
generative adversarial network (GAN) with an improved loss function.
We reimplemented the paper and questioned the purpose of the
discriminator and the suggested loss function.

\section{GANs}
GANs consist of two different networks, a Generator Network and a
Discriminator Network. The concept behind this is that the generative
network estimates a super-resolved image from its LR version with the
goal to become highly similar to real images that the discriminator
network fails to distinguish.

Therefore we optimize the discriminator network $D_{\Theta_D}$ in an
alternating manner along with the generative network $G_{\Theta_G}$ to
solve the adversarial min-max problem:
\begin{align*}
  min_{\Theta_G} max_{\Theta_G} &\mathbb{E}_{I^{HR} \backsim p_{\text{train}}(I^{HR})} [\text{log} D_{\Theta_D}(I^{HR})] \\
  +&\mathbb{E}_{I^{HR} \backsim p_G(I^{LR})} [\text{log} (1-G_{\Theta_G}(I^{LR}))]
\end{align*}
The perceptual loss $l^{SR}$ we defined as weighted sum of a content loss and an discriminative loss component:
\begin{equation*}
  l^{SR}=\alpha l^{SR}_{MSE} + \beta l^{SR}_{VGG16_19/i.j} + \gamma l^{SR}_{D}
\end{equation*}
More precisely, the content loss components are defined as follows:
\begin{align*}
  l^{SR}_{M SE}       &= \frac{1}{r^2WH}\sum_{x=1}^{rW}\sum_{y=1}^{rH}(I^{HR}_{x,y}-G_{\theta_G}(I^{LR})_{x,y})^2 \\
  l^{SR}_{VGG1619/i,j}&=\frac{1}{W_{i,j}H_{i,j}} \sum_{x=1}^{W_{i,j}}\sum_{y=1}^{H_{i,j}}(\Phi_{i,j}(I^{HR})_{x,y}-\Phi_{i,j}(G_{\Theta_G}(I^{LR}))_{x,y}^2
\end{align*}
with downsampling factor $r$ and $W,H$ defining the tensor size.
Finally the discriminative loss is defined as follows
\begin{equation*}  
  l^{SR}_{D}=\sum_{n=1}^N -\text{log}D_{\Theta_D}(G_{\Theta_G}(I^{LR}))
\end{equation*}
\section{Setup}
\label{sec:setup}

% what we used for our work

% \subsection{Dataset}
% \label{sec:data}

For training we used the PASCAL VOC Dataset\cite{pascal-voc-2012} with
more than 10,000 images as well as the NITRE
Dataset\cite{Agustsson_2017_CVPR_Workshops} with 800 images.
% our datasets

% \subsection{Networks}
% \label{sec:nets}

We used pretrained VGG networks in different configurations. The best
results were obtained when we used a VGG1619 network. We also
considered a VGG16 network. Although faster at training it did not
yield results of equal quality.

% the Networks, e.g. VGG16, VGG19, VGG16/19

\section{Results}
\label{sec:results}

% our results => main section
% discriminatro may be omitted
% better nets work better
% ...

% images
We observed that the widely used metrics, SSIM and PSNE, i.\;e.,
structural similarity and peak signal to noise ratio respectively, are
not sufficient for our purposes. This observation was also made by
other authors dealing with image super resolution. Therefore, in
addition to the numerical values of the metrics, we also had to trust
our perception of the images. This is similar to the mean opinion
score (MOS) used in~\cite{LedigChristian2016PSIS}.

For comparison of the generated images with several loss
configurations, i.e. setting the coefficients to zero or using the
values of the paper, see figure~\ref{fig:comp}.
\begin{figure*}[h]
  \centering
  \subcaptionbox{VGG1619 perceptual, adversarial, image loss}[0.2\linewidth]{%
    \includegraphics[width=0.15\linewidth, keepaspectratio]{vgg1619_p_a_i}
  }
  \subcaptionbox{VGG1619 perceptual, adversarial loss}[0.2\linewidth]{%
    \includegraphics[width=0.15\linewidth, keepaspectratio]{vgg1619_p_a}
  }
  \subcaptionbox{VGG1619 perceptual, image loss}[0.2\linewidth]{%
    \includegraphics[width=0.15\linewidth, keepaspectratio]{vgg1619_p_i}
  }
  \subcaptionbox{VGG1619 perceptual loss}[0.2\linewidth]{%
    \includegraphics[width=0.15\linewidth, keepaspectratio]{vgg1619_p}
  }
  \subcaptionbox{VGG1619 adversarial, image loss}[0.2\linewidth]{%
    \includegraphics[width=0.15\linewidth, keepaspectratio]{vgg1619_a_i}
  }
  \subcaptionbox{VGG1619 image loss}[0.2\linewidth]{%
    \includegraphics[width=0.15\linewidth, keepaspectratio]{vgg1619_i}
  }
  \subcaptionbox{VGG19 perception, adversarial, image loss}[0.2\linewidth]{%
    \includegraphics[width=0.15\linewidth, keepaspectratio]{vgg19_p_a_i}
  }
  \subcaptionbox{VGG16 perception, adversarial, image loss}[0.2\linewidth]{%
    \includegraphics[width=0.15\linewidth, keepaspectratio]{vgg16_p_a_i}
  }
  \label{fig:comp}
  \caption{Comparison of several loss configurations}
\end{figure*}

The curves of the PSNR and SSIM values during training may be seen in
figure~\ref{fig:plots}.
\begin{figure*}[h]
  \centering
  \captionsetup{}
  \subcaptionbox{}[0.4\linewidth]{%
    \includegraphics[width=0.4\linewidth, keepaspectratio]{IMG_20180206_113139_679.jpg}
  }
  \subcaptionbox{}[0.4\linewidth]{%
    \includegraphics[width=0.4\linewidth, keepaspectratio]{IMG_20180206_113143_300.jpg}
  }
  \subcaptionbox{}[0.4\linewidth]{%
    \includegraphics[width=0.4\linewidth, keepaspectratio]{IMG_20180206_113147_469.jpg}
  }
  \subcaptionbox{}[0.4\linewidth]{%
    \includegraphics[width=0.4\linewidth, keepaspectratio]{IMG_20180206_113150_955.jpg}
  }
  \subcaptionbox{}[0.4\linewidth]{%
    \includegraphics[width=0.4\linewidth, keepaspectratio]{IMG_20180206_113156_419.jpg}
  }
  \subcaptionbox{}[0.4\linewidth]{%
    \includegraphics[width=0.4\linewidth, keepaspectratio]{IMG_20180206_113200_011.jpg}
  }
  \subcaptionbox{}[0.4\linewidth]{%
    \includegraphics[width=0.4\linewidth, keepaspectratio]{IMG_20180206_113202_817.jpg}
  }
  \subcaptionbox{}[0.4\linewidth]{%
    \includegraphics[width=0.4\linewidth, keepaspectratio]{IMG_20180206_113206_502.jpg}
  }  
  \caption{Curves of metrics during training}
  \label{fig:plots}
\end{figure*}

\section{Conclusion}
\label{sec:conclusion}
One main point is the training. It was very hard to keep the
discriminator and the generator in line. Often we killed our
discriminator such that it could not contribute anymore.

We also observed that super resolution works even without any
discriminator activated, as was suggested by the paper we started
with.

Another key learning point is the quality of the produced images. The
metrics don't change much from some point on but the visual impression
of the generated images still improves.




% something about what we learned

\appendix
%%% Appendix
% include images
% include graphs

{
  \nocite{*}                      % also those references without \cite{Â·}
  \small
  \bibliographystyle{ieee}
  \bibliography{bib}
  
}
\end{document}